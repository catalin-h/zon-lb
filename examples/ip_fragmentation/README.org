#+TITLE: Test zon-lb against IP fragmentation

#+begin_src
┌───────────────────┐      ┌──────────────────────┐      ┌────────────────────┐
│zone1 netns        │      │    default netns     │      │         zone0 netns│
│  server           │      │                      │      │            client  │
│                 ┌─┴──────┴─┐                  ┌─┴──────┴─┐                  │
│            veth3│<-redirect│veth2        veth0│<---------│veth1             │
│         mtu:1310│--------->│mtu:1310  mtu:1500│redirect->│mtu:1500          │
│         xdp:pass└─┬──────┬─┘zon-lb      zon-lb└┬───────┬─┘xdp:pass          │
│tx-checksumminf:off│      │                     │       │tx-checksumming:off │
└───────────────────┘      └─────────────────────┘       └────────────────────┘
#+end_src

Some IP protocols like ICMP or UDP don't have the concept of data segment or
Max Segment Size (MSS) like TCP that prevent IP fragmentation. In such cases,
the IP protocol stack must split the datagram into fragments that fit the MTU.
A load balancer must be able to forward packets that represent IP fragments
without the L4 port or other session id, by only using the fragment information
available at the IP header.

In order for the IP protocol to split the payload into fragments it needs the
Maximum Transmission Unit (MTU) of local interface and the smallest MTU along
the path to the destination endpoint. The first is easy to obtain but the last
one depends if the routers and load balancers along the path are configured to
send Packet Too Big (PTB) IMCP message with the actual MTU. Some applications
can employ the
[[https://datatracker.ietf.org/doc/html/rfc8899][DPLPMTUD]] or
Datagram Packetization Layer Path MTU Discovery.
but this is not always available. Instead the PMUTD should be supported by any
modern OS as it is based on PTB ICMP messages.

For a better integration in the Linux environment the current load balancer
implements both of the above solutions for handling fragments.

** Zon-lb features:

- XDP redirect on veth
- Connection tracking
- FIB lookup
- Full NAT (src/dst L2/L3 addresses change)
- IPv6 fragmentation: track fragments by src+dest+flow-id
- IPv4 fragmentation: track fragments by src+dest+fragment-id+protocol
- IPv6 Path MTU Discovery (PMUTD): send ICMPv6 Packet Too Big message
- IPv4 PMUTD: send ICMPv4 Datagram Too Big message

** Setup

Run the [[./setup.sh][setup]] to create the custom namespace =zone0=:

#+begin_src sh
sudp ./setup.sh [mtu=<MTU>]
#+end_src

The =setup.sh= script accepts an optional key-value parameter for passing the
MTU for the link between =veth2= and =veth3=. Note, that since IPv6 is used in
this test the script will automatically clamp any lower MTU values to the min
IPv6 MTU value of 1280. The MTU can set multiple times without removing the
xdp programs.

Load the configuration [[./zonlb.toml][config]]:

#+begin_src sh
sudo ./zon-lb ./zonlb.toml load
#+end_src

*** Choosing the MTU for the redirect interface
The current test uses a MTU bigger than 1280 which is the minimum MTU
supported by IPv6 - see
[[https://datatracker.ietf.org/doc/html/rfc8200#section-5][Packet Size Issues]].
In fact, on Linux, when setting an MTU smaller than 1280, the system will
automatically remove any IPv6 address set on that interface.

** Test =IP fragmentation= with =ping=
The easiest way to test that fragments are forwarded by LB is to use the =ping=
command by passing a payload size bigger than local MTU to =-s=.

For example, to test IPv4 fragmention, run the command:

#+begin_src sh
sudo ip netns exec zone0 ping -c1 -s2040 10.0.0.1
#+end_src

The output should be something like:

#+begin_src
PING 10.0.0.1 (10.0.0.1) 2040(2068) bytes of data.
2048 bytes from 10.0.0.1: icmp_seq=1 ttl=64 time=0.115 ms

--- 10.0.0.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.115/0.115/0.115/0.000 ms
#+end_src

For IPv6 the command looks similar:
#+begin_src sh
sudo ip netns exec zone0 ping -c1 -s2040 2001:db8::1
#+end_src

If successfull, the output should be like:

#+begin_src
PING 2001:db8::1(2001:db8::1) 2040 data bytes
2048 bytes from 2001:db8::1: icmp_seq=1 ttl=64 time=0.099 ms

--- 2001:db8::1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.099/0.099/0.099/0.000 ms
#+end_src

** Test =PMTUD= with =ping=
To test the Path MTU discovery the ping command must send at least two ICMP
requests as the first one will be erroneous due to receiving the
=Packet/Datagram Too Big= ICMP message.

To make the LB send this ICMP error must first set the MTU for the link between
=veth2= and =veth3= to lower value than the default. Note, that since we want
to test also IPv6 this MTU must at least 1280 - the minimum allowed by IPv6.
Setting the MTU to a lower value on Linux, it will automatically remove =any=
IPv6 address on that link. Since we don't want that the =setup.sh= script
automatically reset the MTU to 1280 if the argument is lower.
So, first update the MTU for link =veth2= and =veth3=:
#+begin_src sh
sudo ./setup.sh mtu=1310
#+end_src

If the script runs without errors we can run the =ping= command next.
To see PMUTD in action the =ping= command provide the =-c<count>= option with
First let's run the ping for IPv4 with =3= retries:
#+begin_src sh
sudo ip netns exec zone0 ping -c3 -s2040 10.0.0.1
#+end_src

The output is interesting as it shows the =remote MTU= after the first ICMP
request:
#+begin_src
PING 10.0.0.1 (10.0.0.1) 2040(2068) bytes of data.
From 10.0.0.1 icmp_seq=1 Frag needed and DF set (mtu = 1310)
2048 bytes from 10.0.0.1: icmp_seq=2 ttl=64 time=0.137 ms
2048 bytes from 10.0.0.1: icmp_seq=3 ttl=64 time=0.115 ms

--- 10.0.0.1 ping statistics ---
3 packets transmitted, 2 received, +1 errors, 33.3333% packet loss, time 2031ms
rtt min/avg/max/mdev = 0.115/0.126/0.137/0.011 ms
#+end_src

Note, that after the first echo request all the subsequent the ICMP requests
are successfull.

Also note, that running the same ping command again will have =all= the
requests successful. This happens because the system caches the route details
for a destination address when it receives the ICMPv4 =Datagram Too Big=
message. The following command will show all the cached routes and the =MTU=:
#+begin_src sh
sudo ip netns exec zone0 ip route show cache
#+end_src

#+begin_src
10.0.0.1 dev veth1
    cache expires 376sec mtu 1310
#+end_src

To trigger =PMUTD= again for IPv4 must =flush= the route cache:
#+begin_src sh
sudo ip netns exec zone0 ip route flush cache
#+end_src

Note, that ICMPv4 uses the term =Datagram Too Big= for the error message type
=3= or =Destination Unreachable=, code =Fragmentation required= and IP header
=DF= flag set. In the ICMPv6 standard, the =Packet Too Big= refers to error
type =2= and code =0=.

To test the PMUTD for IPv6 will use the same =-c<count>= option:
#+begin_src sh
sudo ip netns exec zone0 ping -c3 -s2040 2001:db8::1
#+end_src

#+begin_src
PING 2001:db8::1(2001:db8::1) 2040 data bytes
From 2001:db8::1 icmp_seq=1 Packet too big: mtu=1310
2048 bytes from 2001:db8::1: icmp_seq=2 ttl=64 time=0.131 ms
2048 bytes from 2001:db8::1: icmp_seq=3 ttl=64 time=0.146 ms

--- 2001:db8::1 ping statistics ---
3 packets transmitted, 2 received, +1 errors, 33.3333% packet loss, time 2046ms
rtt min/avg/max/mdev = 0.131/0.138/0.146/0.007 ms
#+end_src

Note, that for IPv6, the output is slightly different and instead of
=Frag needed and DF set= the first message is =Packet too big=. As in case of
IPv4, all the subsequent requests are successful

To show the routes for IPv6 must specify the protocol version with =-6=:
#+begin_src sh
sudo ip netns exec zone0 ip -6 route show cache
#+end_src

#+begin_src
2001:db8::1 dev veth1 metric 256 expires 170sec mtu 1310 pref medium
#+end_src

To trigger the =PMUTD= again for this address flush the =IPv6= route cache:
#+begin_src sh
sudo ip netns exec zone0 ip route flush cache
#+end_src

To stress the LB a little bit more, we will use the =ping flood= mode with
a payload much bigger than the local MTU. The later will suplimentary test that
fragments can also trigger =Packet/Datagram Too Big= ICMP error and that LB
can handle and track multiple ICMP flows (each ICMP request-reply has an unique
identifier).
Note, the =ping flood= mode will also show the PMUTD in action as the =-f=
(flood) option will send echo request indefinitely. However, to limit the
number of requests we will set the total number of retries to =10000=.

The IPv4  =ping flood= command is:
#+begin_src sh
sudo ip netns exec zone0 ping -f -c10000 -s2040 10.0.0.1
#+end_src

Note that the output will show a single =E= for the =Datagram Too Big=
ICMP error:
#+begin_src
PING 10.0.0.1 (10.0.0.1) 2040(2068) bytes of data.
E
--- 10.0.0.1 ping statistics ---
10000 packets transmitted, 9999 received,+1 errors,0.01% packet loss,time 589 ms
rtt min/avg/max/mdev = 0.019/0.034/0.452/0.009 ms,pipe 2,ipg/ewma 0.058/0.037 ms
#+end_src

Note, that in order to see the error or =E= must first flush the route cache
as shown above.

For IPv6 the =ping flood= command is:
#+begin_src sh
sudo ip netns exec zone0 ping -f -c10000 -s2040 2001:db8::1
#+end_src

As for IPv4, the output will show a single =E= for =Packet Too Big= ICMP error:
#+begin_src
PING 2001:db8::1(2001:db8::1) 2040 data bytes
E
--- 2001:db8::1 ping statistics ---
10000 packets transmitted, 9999 received,+1 errors,0.01% packet loss, time 674ms
rtt min/avg/max/mdev = 0.022/0.042/0.234/0.010 ms, ipg/ewma 0.067/0.043 ms
#+end_src

Note, that for both IPv4 and IPv6 =ping flood= the first request is used to
update the MTU and all the =9999= subsequence IMCP requests finish without
error. For =ping= this means that the entire payload was transferred ok and
without ICMP flow interference.

** Notes about TCP (=SOCK_STREAM=) and UDP (=SOCK_DGRAM=)
On Linux, one of the socket options that can be set is =IP_MTU_DISCOVER=.
The [[https://man7.org/linux/man-pages/man7/ip.7.html][ip]] docs mention:

#+begin_src
When enabled, Linux will perform Path MTU
Discovery as defined in RFC 1191 on SOCK_STREAM sockets.
For non-SOCK_STREAM sockets, IP_PMTUDISC_DO forces the
don't-fragment flag to be set on all outgoing packets.  It
is the user's responsibility to packetize the data in MTU-
ized chunks and to do the retransmits if necessary.

The system-wide default can be toggled between
IP_PMTUDISC_WANT and IP_PMTUDISC_DONT by writing
(respectively, zero and nonzero values) to the
/proc/sys/net/ipv4/ip_no_pmtu_disc file.
Path MTU discovery value   Meaning
IP_PMTUDISC_WANT           Use per-route settings.
IP_PMTUDISC_DONT           Never do Path MTU Discovery.
IP_PMTUDISC_DO             Always do Path MTU Discovery.
IP_PMTUDISC_PROBE          Set DF but ignore Path MTU.

When PMTU discovery is enabled, the kernel automatically
keeps track of the path MTU per destination host.  When it
is connected to a specific peer with connect(2), the
currently known path MTU can be retrieved conveniently
using the IP_MTU socket option (e.g., after an EMSGSIZE
error occurred).  The path MTU may change over time.  For
connectionless sockets with many destinations, the new MTU
for a given destination can also be accessed using the
error queue (see IP_RECVERR).  A new error will be queued
for every incoming MTU update.

While MTU discovery is in progress, initial packets from
datagram sockets may be dropped.  Applications using UDP
should be aware of this and not take it into account for
their packet retransmit strategy.

To bootstrap the path MTU discovery process on unconnected
sockets, it is possible to start with a big datagram size
(headers up to 64 kilobytes long) and let it shrink by
updates of the path MTU.

To get an initial estimate of the path MTU, connect a
datagram socket to the destination address using
connect(2) and retrieve the MTU by calling getsockopt(2)
with the IP_MTU option.

It is possible to implement RFC 4821 MTU probing with
SOCK_DGRAM or SOCK_RAW sockets by setting a value of
IP_PMTUDISC_PROBE (available since Linux 2.6.22).  This is
also particularly useful for diagnostic tools such as
tracepath(8) that wish to deliberately send probe packets
larger than the observed Path MTU.
#+end_src

On the other hand, the [[https://man7.org/linux/man-pages/man7/udp.7.html][udp]]
manual explains that:
#+begin_src
By default, Linux UDP does path MTU (Maximum Transmission Unit)
discovery. This means the kernel will keep track of the MTU to a
specific target IP address and return EMSGSIZE when a UDP packet
write exceeds it. When this happens, the application should
decrease the packet size. Path MTU discovery can be also turned
off using the IP_MTU_DISCOVER socket option or the
/proc/sys/net/ipv4/ip_no_pmtu_disc file; see ip(7) for details.
When turned off, UDP will fragment outgoing UDP packets that
exceed the interface MTU. However, disabling it is not
recommended for performance and reliability reasons.
#+end_src

*** Undestanding when =ip route show cache= shows the route =mtu=

As noted in the [[http://linux-ip.net/html/routing-cache.html][iproute:Routing Cache]]
the MTU size is kept as a per route attribute and is stored in the routing cache.
However, as of kernel version 3.6 the routing cache was removed
([[https://lwn.net/Articles/507651/][LWN article]])
and replaced with a Forwarding Informational Base (FIB) trie.

With this change the =ip route show cache= doesn't always show the MTU for
a cached entry. This happens because the route MTU property is stored in
the kernel FIB NextHop Exceptions data structure and only updated after ICMP
messages like Redirect or Datagram Too Big as noted in
[[https://nscpolteksby.ac.id/ebook/files/Ebook/Computer%20Engineering/Linux%20Kernel%20Networking%20-%20Implementation%20(2014)/chapter%205%20The%20IPv4%20Routing%20Subsystem.pdf][Linux Kernel Networking - Implementation (2014): The IPv4 Routing Subsystem: FIB Nexthop Exceptions]]:
#+begin_src
Caching of Path MTU and ICMPv4 redirects is done with FIB exceptions
#+end_src
#+begin_src
FIB nexthop exceptions were added in kernel 3.6 to handle cases when a routing entry is changed not as a result of a
userspace action, but as a result of an ICMPv4 Redirect message or as a result of Path MTU discovery.
#+end_src
#+begin_src
Caching of Path MTU and ICMPv4 redirects is done with FIB exceptions
The second case of generating FIB nexthop exceptions is when the Path MTU has changed, in the __ip_rt_
update_pmtu() method. In such a case, the fnhe_pmtu field of the fib_nh_exception object is set to be the new MTU
when creating the FIB nexthop exception object (in the update_or_create_fnhe() method). PMTU value is expired if
it was not updated in the last 10 minutes (ip_rt_mtu_expires). This period is checked on every dst_mtu() call via the
ipv4_mtu() method, which is a dst->ops->mtu handler. The ip_rt_mtu_expires, which is by default 600 seconds, can
be configured via the procfs entry /proc/sys/net/ipv4/route/mtu_expires
#+end_src

The above statements can be easily tested if we open a UDP connection with
=netcat= and try to send a message bigger than the remote =MTU=:
#+begin_src sh
sudo ip netns exec zone1 nc -unlv -p 223 -s 10.2.0.2
#+end_src
#+begin_src sh
sudo ip netns exec zone0 nc -u 10.0.0.1 23
#+end_src

When checking the cached routes in the netns =zone0= we should see the new route
and the =mtu= property changed to remote MTU:
#+begin_src sh
ip -s -d route show cache
#+end_src
#+begin_src
unicast 10.0.0.1 dev veth1
    cache expires 594sec users 1 age 5sec mtu 1340
#+end_src
Note that by passing =-s= the commands shows other interesting info like
the number of users (sockets) and the expiration time.

*** Trigger the =Path MTU= discovery using =UDP=

To verify the UDP Packet/Datagram Too Big we can use tools like =traceroute=
or =tracepath=.

To use =traceroute= for IPv4 path MTU checking run:
#+begin_src sh
sudo ip netns exec zone0 traceroute --mtu -F -N1 -f1 -m1 -U -p 23 10.0.0.1
#+end_src

For current setup the result should be something like:
#+begin_src sh
traceroute to 10.0.0.1 (10.0.0.1), 1 hops max, 65000 byte packets
 1  * F=1320 * *
#+end_src

To run =traceroute= for IPv6 path MTU discovery:
#+begin_src sh
sudo ip netns exec zone0 traceroute --mtu -F -N1 -f1 -m1 -U -p 23 2001:db8::1
#+end_src

The output should look like:
#+begin_src sh
traceroute to 2001:db8::1 (2001:db8::1), 1 hops max, 65000 byte packets
 1  * F=1320 * *
#+end_src

For =traceroute= make sure to pass the =-U= parameter in order to send UDP
probesc and =-F= to set the =don't fragment= flag. Also, the tool supports
sending ICMP probes by passing the =-I= option.

The =tracepath= tool is more simple and sends only UDP probes.

An usage example for IPv4 is:
#+begin_src sh
sudo ip netns exec zone0 tracepath -n -p 23 10.0.0.1
#+end_src

For current setup the output looks like:
#+begin_src sh
 1?: [LOCALHOST]                      pmtu 1500
 1:  10.0.0.1                                              0.088ms pmtu 1320
 1:  10.0.0.1                                              0.051ms reached
     Resume: pmtu 1320 hops 1 back 1
#+end_src

Checking the path MTU IPv6 is done with:
#+begin_src sh
sudo ip netns exec zone0 tracepath -6 -n -p 23 2001:db8::1
#+end_src

The output for IPv6 looks like:
#+begin_src sh
 1?: [LOCALHOST]                        0.037ms pmtu 1500
 1:  2001:db8::1                                           0.162ms pmtu 1320
 1:  2001:db8::1                                           0.087ms reached
     Resume: pmtu 1320 hops 1 back 1
#+end_src

Note, that both tool will not send any probes if there a cached route with
the MTU already updated. To make sure probes are sent make sure the routing
cache is flushed - see the commands above.

*** How =Path MTU= discovery works for =TCP=

The =good= part about using UDP for path discovery is that
there is no need to launch a server as the first UDP packet that is bigger than
the next hop MTU gets the packet too big error.

For TCP, the MTU is usually computed during the 3-way handshake using the TCP
options. The endpoint that initiates the connection sends a TCP option with
the supported MSS (MTU=MSS+IP header size) and the receiving endpoint replies
with the supported MSS. The bellow 3-way handshake packet capture is self
explanatory:
#+begin_src
sudo ip netns exec zone0 tshark -c3 tcp
Running as user "root" and group "root". This could be dangerous.
Capturing on 'veth1'
 ** (tshark:597378) 21:56:59.946852 [Main MESSAGE] -- Capture started.
 ** (tshark:597378) 21:56:59.947576 [Main MESSAGE] -- File: "/tmp/wireshark_veth18GQAT2.pcapng"
    1 0.000000000     10.0.0.2 → 10.0.0.1     TCP 74 53340 → 23 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM TSval=1943992280 TSecr=0 WS=128
    2 0.000097168     10.0.0.1 → 10.0.0.2     TCP 74 23 → 53340 [SYN, ACK] Seq=0 Ack=1 Win=64400 Len=0 MSS=1300 SACK_PERM TSval=242237054 TSecr=1943992280 WS=128
    3 0.000118192     10.0.0.2 → 10.0.0.1     TCP 66 53340 → 23 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=1943992280 TSecr=242237054
#+end_src

With the current setup is not possible to trigger a packet too big error and
see the Path MTU discovery mechanism in action for TCP. With the current setup
only the LB can trigger a PTB if the backend can be configured with a custom MTU.

** References

- [[https://labs.ripe.net/author/gih/evaluating-ipv4-and-ipv6-packet-fragmentation/][Evaluating IPv4 and IPv6 Packet Fragmentation]]
- [[https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt][ip sysctl - Path MTU discovery settings]]
- [[https://packetpushers.net/blog/ip-fragmentation-in-detail/][IP Fragmentation in Detail]]
- [[https://lwn.net/Articles/960913/][So you think you understand IP fragmentation?]]
- [[https://datatracker.ietf.org/doc/html/rfc6436][Rationale for Update to the IPv6 Flow Label Specification]]
- [[http://linux-ip.net/html/tools-ip-route.html][ip route manual]]
- [[https://datatracker.ietf.org/doc/html/rfc8899][Datagram Packetization Layer Path MTU Discovery]]
- [[https://man7.org/linux/man-pages/man7/ip.7.html][ip - IP_MTU_DISCOVER]]
- [[https://lartc.org/lartc.html][Linux Advanced Routing & Traffic Control HOWTO]]

